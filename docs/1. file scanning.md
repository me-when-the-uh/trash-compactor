# File Scanning

Trash-compactor identifies compressible files through a three-stage pipeline: discovery, filtering, and planning. The process begins in `main.py` and branches into specific modes (standard compression, dry-run, or legacy branding).

## 1. Discovery

The `iter_files` function in `src/compression/compression_planner.py` traverses the directory tree. It yields file paths to the planner while filtering directories in place.

### Directory Filtering

Before entering a directory, `maybe_skip_directory` (in `src/skip_logic.py`) evaluates it against two criteria.

First, it checks if the path matches known Windows system directories defined in `src/config.py`, such as `C:\Windows`. This prevents accidental modification of critical system files.

Second, if entropy analysis is enabled, the program samples directory content to estimate compressibility. Directories with high entropy, indicating low potential savings, are excluded from traversal. Skipped directories are recorded in `CompressionStats`.

## 2. Planning

The `plan_compression` function receives file entries and determines eligibility.

### 2.1. File Selection

The `should_compress_file` function in `src/file_utils.py` validates each file. It excludes formats that are already compressed, such as `.zip` or `.jpg` files. It also ignores files smaller than `MIN_COMPRESSIBLE_SIZE` (default 4KB) and checks NTFS attributes to ensure the file is not already compressed.

**Performance Note:** The scanning phase is optimized to run on a single thread. The overhead of managing concurrent workers for such lightweight metadata checks (using cached `os.DirEntry` stats) outweighs the benefits of parallelism.

### 2.2. Algorithm Selection

Eligible files are assigned a compression algorithm based on size (`src/config.py`). Files smaller than 64KB use XPRESS4K. Files between 64KB and 256KB use XPRESS8K. The program applies XPRESS16K for files up to 1MB, and uses LZX for files larger than 1MB.

### 2.3. Entropy Filtering

`_filter_high_entropy_directories` performs a second pass. It aggregates candidates by parent directory and samples entropy. If a directory's estimated savings fall below the threshold, the program removes all candidates belonging to that directory.

## 3. Entropy Sampling

The `sample_directory_entropy` function in `src/compression/entropy.py` estimates compressibility without full processing.

It uses reservoir sampling to select a random subset of files (default 50) from the directory tree. For each selected file, it reads up to three 16KB windows from the start, middle, and end of the file. These samples are compressed with zlib (level 1), and the resulting ratio approximates the file's entropy. This in turn helps filter out files that already have a compressed structure - they will fail the entropy analysis in all three spots: the start, the middle and the end. If a file is not compressed, entropy analysis will show that it compresses easily in all three spots. If a file is already compressed in 2 out of 3 spots, then the compression likely won't yield good results. This approach to analysing files helps achieve the necessary accuracy without reducing performance. When large directory trees are involved, sampling tasks are also distributed across the same worker pool to keep analysis time predictable.

The `savings_from_entropy` function then converts this entropy score to a projected savings percentage during `--dry-run` mode.

## 4. Execution

The planner returns a list of tuples containing the path, size, and algorithm. The `execute_compression_plan` function distributes these tasks to worker pools, which are scaled according to the system's CPU topology (`src/workers.py`).